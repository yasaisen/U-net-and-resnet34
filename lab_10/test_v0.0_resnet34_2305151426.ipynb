{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Import"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","def visualize(**images):\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","\n","def round(temp):\n","    return np.round((temp - np.min(temp))/((np.max(temp) - np.min(temp))))\n","\n","def yasai_show_v1(dataset, idx, model=None):\n","    image, mask = dataset[idx]\n","    if model is not None:\n","        pred = model(image.unsqueeze(0))\n","        with torch.no_grad():\n","            pred = np.asarray(pred).squeeze()\n","    with torch.no_grad():\n","        image = np.asarray(image).transpose(1, 2, 0)\n","        mask = np.asarray(mask)\n","\n","    if model is not None:\n","        tempdict = {}\n","        tempdict['image'] = image\n","        for i in range(pred.shape[0]):\n","            tempdict['pred_' + str(i)] = 0.4 * round(pred[i]) + 0.6 * image[...,0].squeeze()\n","        visualize(**tempdict)\n","\n","    tempdict = {}\n","    tempdict['image'] = image\n","    for i in range(mask.shape[0]):\n","        tempdict['mask_' + str(i)] = 0.4 * round(mask[i]) + 0.6 * image[...,0].squeeze()\n","    visualize(**tempdict)\n","\n","def yasai_model_save_v1(model, text=''):\n","    temp = os.path.join(os.getcwd(), 'model_' + text + datetime.now().strftime(\"%y%m%d%H%M.pt\"))\n","    torch.save({'state_dict': model.state_dict(), 'model': model}, temp)\n","    print('Successfully saved to ' + temp)\n","\n","def yasai_model_load_v1(path):\n","    temp = torch.load(path)\n","    model = temp['model']\n","    model.load_state_dict(temp['state_dict'])\n","    print('Successfully loaded from ' + path)\n","    return model\n","\n","def yasai_compute_iou_v1(pred, label):\n","    # print(label.shape, np.unique(label))\n","    # print(round(pred).shape, np.unique(round(pred)))\n","    label_c = label == 1\n","    pred_c = round(pred) == 1\n","\n","    intersection = np.logical_and(pred_c, label_c).sum()\n","    union = np.logical_or(pred_c, label_c).sum()\n","\n","    if union != 0 and np.sum(label_c) != 0:\n","        return intersection / union\n","    \n","def yasai_compute_batch_iou_v1(model, data_loader):\n","    ious = []\n","    for image, mask in tqdm(data_loader, desc='Iterating'):\n","        pred = model(image)\n","        with torch.no_grad():\n","            pred = np.asarray(pred).squeeze()\n","            mask = np.asarray(mask)\n","        ious += [yasai_compute_iou_v1(pred, mask)]\n","    print(sum(ious)/len(ious))"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torchvision\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import pandas as pd\n","from torchvision import transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","from PIL import Image\n","import torch.nn.functional as F\n","import cv2\n","# from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Root"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["ROOT_PATH = '/home/yasaisen/Desktop/09_research/09_research_main/lab_03'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["dataset_folder = 'dataset_C_v_2.9.3'\n","\n","train_img_path = os.path.join(ROOT_PATH, dataset_folder, 'train_for_base_imgs_rgb')\n","train_mask_path = os.path.join(ROOT_PATH, dataset_folder, 'train_for_base_mask')\n","\n","valid_img_path = os.path.join(ROOT_PATH, dataset_folder, 'valid_imgs_rgb')\n","valid_mask_path = os.path.join(ROOT_PATH, dataset_folder, 'valid_mask')\n","\n","test_img_path = os.path.join(ROOT_PATH, dataset_folder, 'test_imgs_rgb')\n","test_mask_path = os.path.join(ROOT_PATH, dataset_folder, 'test_mask')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Aug"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["img_size = 224\n","train_bsz = 32\n","device = 'cuda'\n","epochs = 30\n","valid_bsz = 8\n","test_bsz = 8"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_df(img_path, mask_path):\n","    images, labels = [], []\n","\n","    i = 0\n","\n","    for get_img_name in os.listdir(img_path):\n","        images += [os.path.join(img_path, get_img_name)] # NORMAL_G1_Lid1_LRid293_Gid3133_Bl30.png\n","        labels += [get_img_name.split('_')[0]]\n","        \n","        i = i+1\n","\n","    PathDF = pd.DataFrame({'images': images, 'labels': labels})\n","    print(i)\n","    PathDF.head()\n","    return PathDF"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3771\n","1037\n","917\n"]}],"source":["train_df = get_df(train_img_path, train_mask_path)\n","valid_df = get_df(valid_img_path, valid_mask_path)\n","test_df = get_df(test_img_path, test_mask_path)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n","            transforms.ToTensor()\n","            ])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# mask_path = '/home/yasaisen/Desktop/09_research/09_research_main/lab_03/dataset_C_v_2.9.3/train_for_base_mask/RSLN_L_G10_Lid45_LRid112_Gid7024_C4.png'\n","# label = Image.open(mask_path)\n","# label = np.array(label)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class mod_Dataset(Dataset):\n","    def __init__(self, path_df, transform=None):\n","        self.path_df = path_df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.path_df.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        if self.transform is not None:\n","            trans_Resize = transforms.Resize(224)\n","\n","            images = trans_Resize(Image.open(self.path_df.iloc[idx]['images']).convert('RGB'))\n","            images = self.transform(images)\n","\n","            if self.path_df.iloc[idx]['labels'] == 'NORMAL':\n","                lables = torch.tensor([1, 0, 0], dtype=torch.float32)\n","            if self.path_df.iloc[idx]['labels'] == 'RLN':\n","                lables = torch.tensor([0, 1, 0], dtype=torch.float32)\n","            if self.path_df.iloc[idx]['labels'] == 'RSLN':\n","                lables = torch.tensor([0, 0, 1], dtype=torch.float32)\n","\n","        return images, lables"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["train_data = mod_Dataset(train_df, transform)\n","valid_data = mod_Dataset(valid_df, transform)\n","test_data  = mod_Dataset(test_df, transform)\n","\n","train_loader = DataLoader(train_data, batch_size=train_bsz, shuffle=True , num_workers=0, pin_memory=True, drop_last=True)\n","valid_loader = DataLoader(valid_data, batch_size=valid_bsz, shuffle=False, num_workers=0)\n","test_loader  = DataLoader(test_data , batch_size=test_bsz , shuffle=False, num_workers=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class resnet34(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.base_model = torchvision.models.resnet34(weights=None)\n","        self.fc1 = nn.Linear(1000, num_classes)\n","\n","    def forward(self, input):\n","        output = self.base_model(input)\n","        output = self.fc1(output)\n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 3, 224, 224])\n","torch.Size([4, 3])\n","torch.Size([32, 3, 224, 224])\n","torch.Size([32, 3])\n"]}],"source":["model = resnet34(3).to(device)\n","# print(model)\n","t = torch.randn((4, 3, 224, 224)).to(device)\n","print(t.shape)\n","get = model(t)\n","print(get.shape)\n","\n","for x, y in train_loader:\n","    print(x.shape)\n","    print(y.shape)\n","    break"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def check_accuracy(loader, model, device):\n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","            _, y = y.max(1)\n","            num_correct += (predictions == y).sum()\n","            num_samples += predictions.size(0)\n","\n","    model.train()\n","    return (num_correct/num_samples).item()\n","\n","def train(epochs, model):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        losses = []\n","\n","        pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch}\")\n","        for data, targets in pbar:\n","            data = data.to(device)\n","            targets = targets.to(device)\n","\n","            # forward\n","            scores = model(data)\n","            loss = criterion(scores, targets)\n","            losses.append(loss.item())\n","            # backward\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            optimizer.step()\n","        \n","        avg_loss = sum(losses) / len(losses)\n","        acc = check_accuracy(test_loader, model, device)\n","        print(f\"Loss:{avg_loss:.8f}\\tAccuracy:{acc:.8f}\")\n","\n","    return model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 0: 100%|██████████| 117/117 [01:37<00:00,  1.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.36553614\tAccuracy:0.34569246\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 117/117 [01:32<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.05602545\tAccuracy:0.32824427\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 117/117 [01:30<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.05395426\tAccuracy:0.42639038\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.02843638\tAccuracy:0.27480915\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 117/117 [01:31<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00343214\tAccuracy:0.29770991\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 117/117 [01:30<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00103095\tAccuracy:0.30643401\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 117/117 [01:33<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00071755\tAccuracy:0.28353325\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 117/117 [01:30<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00029976\tAccuracy:0.30752453\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 117/117 [01:33<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00020224\tAccuracy:0.30316249\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 117/117 [01:32<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00013655\tAccuracy:0.28571427\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 117/117 [01:32<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00009814\tAccuracy:0.28680480\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11: 100%|██████████| 117/117 [01:31<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00007442\tAccuracy:0.28571427\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12: 100%|██████████| 117/117 [01:31<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00014898\tAccuracy:0.28026173\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13: 100%|██████████| 117/117 [01:30<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00043606\tAccuracy:0.27044711\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00037538\tAccuracy:0.36859322\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 15: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.03340307\tAccuracy:0.44383860\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 16: 100%|██████████| 117/117 [01:30<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.09908101\tAccuracy:0.41875678\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 17: 100%|██████████| 117/117 [01:30<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.01649511\tAccuracy:0.51799345\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 18: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.02305389\tAccuracy:0.32933477\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 19: 100%|██████████| 117/117 [01:31<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.03440163\tAccuracy:0.36205015\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 20: 100%|██████████| 117/117 [01:31<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00170275\tAccuracy:0.30098146\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 21: 100%|██████████| 117/117 [01:33<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00049440\tAccuracy:0.31188658\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 22: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00053530\tAccuracy:0.32606325\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 23: 100%|██████████| 117/117 [01:32<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.01555282\tAccuracy:0.39149398\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 24: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.01940561\tAccuracy:0.29880041\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 25: 100%|██████████| 117/117 [01:30<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00443666\tAccuracy:0.36205015\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 26: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.02051344\tAccuracy:0.40676117\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 27: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00874591\tAccuracy:0.42420936\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 28: 100%|██████████| 117/117 [01:31<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00069688\tAccuracy:0.41112322\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 29: 100%|██████████| 117/117 [01:32<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss:0.00032586\tAccuracy:0.41330424\n"]}],"source":["trained_resnet34_model = train(30, model)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully saved to /home/yasaisen/Desktop/09_research/09_research_main/lab_10/model_resnet34_trainbasergb_2305151701.pt\n"]}],"source":["yasai_model_save_v1(trained_resnet34_model, 'resnet34_trainbasergb_')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}}},"nbformat":4,"nbformat_minor":0}
