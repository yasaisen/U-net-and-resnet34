{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import pandas as pd\n","from torchvision import transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","from PIL import Image\n","import torch.nn.functional as F\n","import cv2\n","# from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Root"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ROOT_PATH = '/home/yasaisen/Desktop/09_research/09_research_main/lab_03'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_folder = 'dataset_C_v_2.9.3'\n","\n","train_img_path = os.path.join(ROOT_PATH, dataset_folder, 'train_for_base_imgs')\n","train_mask_path = os.path.join(ROOT_PATH, dataset_folder, 'train_for_base_mask')\n","\n","valid_img_path = os.path.join(ROOT_PATH, dataset_folder, 'valid_imgs')\n","valid_mask_path = os.path.join(ROOT_PATH, dataset_folder, 'valid_mask')\n","\n","test_img_path = os.path.join(ROOT_PATH, dataset_folder, 'test_imgs')\n","test_mask_path = os.path.join(ROOT_PATH, dataset_folder, 'test_mask')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Aug"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img_size = 224\n","train_bsz = 4\n","device = 'cuda'\n","epochs = 30\n","valid_bsz = 8\n","test_bsz = 8"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_df(img_path, mask_path):\n","    images, masks = [], []\n","\n","    i = 0\n","\n","    for get_img_name in os.listdir(img_path):\n","        images += [os.path.join(img_path, get_img_name)] # NORMAL_G1_Lid1_LRid293_Gid3133_Bl30.png\n","        masks += [os.path.join(mask_path, get_img_name.replace(get_img_name.split('_')[-1], 'C4.png'))] # NORMAL_G1_Lid1_LRid293_Gid3133_C4.png\n","        \n","        i = i+1\n","\n","    PathDF = pd.DataFrame({'images': images, 'masks': masks})\n","    print(i)\n","    PathDF.head()\n","    return PathDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df = get_df(train_img_path, train_mask_path)\n","valid_df = get_df(valid_img_path, valid_mask_path)\n","test_df = get_df(test_img_path, test_mask_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_example(idx, df):\n","    image_path = df['images'].iloc[idx]\n","    mask_path = df['masks'].iloc[idx]\n","    image = Image.open(image_path)#.convert('RGB')\n","    mask = Image.open(mask_path)#.convert('RGB')\n","    \n","    fig, ax = plt.subplots(1, 3, figsize=(8,4))\n","    ax[0].imshow(np.array(image).astype(np.uint8))\n","    ax[0].set_title(\"Image\")\n","    ax[1].imshow(np.array(mask).astype(np.uint8))\n","    ax[1].set_title(\"Mask\")\n","    img = np.array(image) * 0.3 + np.array(mask) * 0.7\n","    img = img.astype(np.uint8)\n","    ax[2].imshow(img)\n","    ax[2].set_title('')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_example(0, train_df)\n","plot_example(50, valid_df)\n","plot_example(88, test_df)\n","plot_example(190, train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n","            transforms.ToTensor()\n","            ])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# mask_path = '/home/yasaisen/Desktop/09_research/09_research_main/lab_03/dataset_C_v_2.9.3/train_for_base_mask/RSLN_L_G10_Lid45_LRid112_Gid7024_C4.png'\n","# label = Image.open(mask_path)\n","# label = np.array(label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class mod_Dataset(Dataset):\n","    def __init__(self, path_df, transform=None):\n","        self.path_df = path_df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.path_df.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        if self.transform is not None:\n","            trans_Resize = transforms.Resize(224)\n","\n","            images = trans_Resize(Image.open(self.path_df.iloc[idx]['images']).convert('RGB'))\n","            images = self.transform(images)\n","\n","            mask = trans_Resize(Image.open(self.path_df.iloc[idx]['masks']))\n","            mask = np.array(mask)\n","            masks = np.zeros([np.max(np.unique(mask))+1, mask.shape[0], mask.shape[1]])\n","            for i in range(mask.shape[0]):\n","                for j in range(mask.shape[1]):\n","                    masks[mask[i][j]][i][j] = 1\n","\n","            masks = torch.from_numpy(masks)\n","            masks = masks.type(torch.float32)\n","\n","        return images, masks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = mod_Dataset(train_df, transform)\n","valid_data = mod_Dataset(valid_df, transform)\n","test_data  = mod_Dataset(test_df, transform)\n","\n","train_loader = DataLoader(train_data, batch_size=train_bsz, shuffle=True , num_workers=0, pin_memory=True, drop_last=True)\n","valid_loader = DataLoader(valid_data, batch_size=valid_bsz, shuffle=False, num_workers=0)\n","test_loader  = DataLoader(test_data , batch_size=test_bsz , shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","def visualize(**images):\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","\n","def round(temp):\n","    return np.round((temp - np.min(temp))/((np.max(temp) - np.min(temp))))\n","\n","def yasai_show_v2(dataset, idx, model=None, label=False):\n","    image, mask = dataset[idx]\n","    if model is not None:\n","        pred = model(image.unsqueeze(0))\n","        with torch.no_grad():\n","            pred = np.asarray(pred).squeeze()\n","    with torch.no_grad():\n","        image = np.asarray(image).transpose(1, 2, 0)\n","        mask = np.asarray(mask)\n","\n","    if model is not None:\n","        tempdict = {}\n","        tempdict['image'] = image\n","        for i in range(pred.shape[0]):\n","            tempdict['pred_' + str(i)] = 0.4 * round(pred[i]) + 0.6 * image[...,0].squeeze()\n","        visualize(**tempdict)\n","\n","    if label:\n","        tempdict = {}\n","        tempdict['image'] = image\n","        for i in range(mask.shape[0]):\n","            tempdict['mask_' + str(i)] = 0.4 * round(mask[i]) + 0.6 * image[...,0].squeeze()\n","        visualize(**tempdict)\n","\n","def yasai_model_save_v1(model, text=''):\n","    temp = os.path.join(os.getcwd(), 'model_' + text + datetime.now().strftime(\"%y%m%d%H%M.pt\"))\n","    torch.save({'state_dict': model.state_dict(), 'model': model}, temp)\n","    print('Successfully saved to ' + temp)\n","\n","def yasai_model_load_v1(path):\n","    temp = torch.load(path)\n","    model = temp['model']\n","    model.load_state_dict(temp['state_dict'])\n","    print('Successfully loaded from ' + path)\n","    return model\n","\n","def yasai_compute_iou_v1(pred, label):\n","    # print(label.shape, np.unique(label))\n","    # print(round(pred).shape, np.unique(round(pred)))\n","    label_c = label == 1\n","    pred_c = round(pred) == 1\n","\n","    intersection = np.logical_and(pred_c, label_c).sum()\n","    union = np.logical_or(pred_c, label_c).sum()\n","\n","    if union != 0 and np.sum(label_c) != 0:\n","        return intersection / union\n","    \n","def yasai_compute_batch_iou_v1(model, data_loader):\n","    ious = []\n","    for image, mask in tqdm(data_loader, desc='Iterating'):\n","        pred = model(image)\n","        with torch.no_grad():\n","            pred = np.asarray(pred).squeeze()\n","            mask = np.asarray(mask)\n","        ious += [yasai_compute_iou_v1(pred, mask)]\n","    print(sum(ious)/len(ious))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["yasai_show_v1(train_data, 1, None)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","  def __init__(self, in_channels, middle_channels, out_channels):\n","    super(Decoder, self).__init__()\n","    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n","    self.conv_relu = nn.Sequential(\n","        nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n","        nn.ReLU(inplace=True)\n","        )\n","  def forward(self, x1, x2):\n","    x1 = self.up(x1)\n","    x1 = torch.cat((x1, x2), dim=1)\n","    x1 = self.conv_relu(x1)\n","    return x1\n","\n","class Unet(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        self.base_model = torchvision.models.resnet18(weights='DEFAULT')\n","        self.base_layers = list(self.base_model.children())\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n","            self.base_layers[1],\n","            self.base_layers[2])\n","        self.layer2 = nn.Sequential(*self.base_layers[3:5])\n","        self.layer3 = self.base_layers[5]\n","        self.layer4 = self.base_layers[6]\n","        self.layer5 = self.base_layers[7]\n","        self.decode4 = Decoder(512, 256+256, 256)\n","        self.decode3 = Decoder(256, 256+128, 256)\n","        self.decode2 = Decoder(256, 128+64, 128)\n","        self.decode1 = Decoder(128, 64+64, 64)\n","        self.decode0 = nn.Sequential(\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","            nn.Conv2d(64, 32, kernel_size=3, padding=1, bias=False),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n","            )\n","        self.conv_last = nn.Conv2d(64, n_class, 1)\n","\n","    def forward(self, input):\n","        e1 = self.layer1(input) # 64,128,128\n","        e2 = self.layer2(e1) # 64,64,64\n","        e3 = self.layer3(e2) # 128,32,32\n","        e4 = self.layer4(e3) # 256,16,16\n","        f = self.layer5(e4) # 512,8,8\n","        d4 = self.decode4(f, e4) # 256,16,16\n","        d3 = self.decode3(d4, e3) # 256,32,32\n","        d2 = self.decode2(d3, e2) # 128,64,64\n","        d1 = self.decode1(d2, e1) # 64,128,128\n","        d0 = self.decode0(d1) # 64,256,256\n","        out = self.conv_last(d0) # 1,256,256\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = yasai_model_load_v1('/home/yasaisen/Desktop/09_research/09_research_main/lab_10/model_bast_ima_2305151053.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import segmentation_models_pytorch as smp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = smp.Unet(\n","#     in_channels=3,\n","#     classes=4,\n","#     activation=\"softmax\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = Unet(4).to(device)\n","# # print(model)\n","# t = torch.randn((4, 3, 224, 224)).to(device)\n","# print(t.shape)\n","# get = model(t)\n","# print(get.shape)\n","\n","# for x, y in train_loader:\n","#     print(x.shape)\n","#     print(y.shape)\n","#     break"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=2, logits=False, reduction=True):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.logits = logits\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        if self.logits:\n","            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        else:\n","            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","\n","        if self.reduction:\n","            return torch.mean(F_loss)\n","        else:\n","            return F_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_coef_metric(pred, label):\n","    intersection = 2.0 * (pred * label).sum()\n","    union = pred.sum() + label.sum()\n","    if pred.sum() == 0 and label.sum() == 0:\n","        return 1\n","    return intersection / union"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_loop(model, optimizer, criterion, train_loader, device=device):\n","    running_loss = 0\n","    model.train()\n","    pbar = tqdm(train_loader, desc='Iterating over train data')\n","\n","    final_dice_coef = 0 \n","    \n","    for imgs, masks in pbar:\n","        # pass to device\n","\n","        # print(type(imgs), imgs.shape)\n","        # print(type(masks), masks.shape)\n","\n","        imgs = imgs.to(device)\n","        masks = masks.to(device)\n","\n","        # forward\n","        out = model(imgs)\n","        loss = criterion(out, masks)\n","        running_loss += loss.item() * imgs.shape[0]\n","#         print(loss.item())\n","        \n","        out_cut = np.copy(out.detach().cpu().numpy())\n","        out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n","        out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n","            \n","        train_dice = dice_coef_metric(out_cut, masks.data.cpu().numpy())\n","        final_dice_coef += train_dice \n","        \n","        # optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    running_loss /= len(train_loader.sampler)\n","    return {'dice coef':final_dice_coef/len(train_loader), \n","                'loss':running_loss}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def eval_loop(model, criterion, eval_loader, device=device):\n","    \n","    running_loss = 0\n","    final_dice_coef = 0 \n","    \n","    model.eval()\n","    with torch.no_grad():\n","\n","        pbar = tqdm(eval_loader, desc='Interating over evaluation data')\n","        \n","        for imgs, masks in pbar:\n","            \n","            imgs = imgs.to(device)\n","            masks = masks.to(device)\n","            \n","            out = model(imgs)\n","            loss = criterion(out, masks)\n","            running_loss += loss.item() * imgs.shape[0]\n","#             print(loss.item())\n","            \n","            out_cut = np.copy(out.detach().cpu().numpy())\n","            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n","            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n","            \n","            valid_dice = dice_coef_metric(out_cut, masks.data.cpu().numpy())\n","            final_dice_coef += valid_dice \n","            \n","    running_loss /= len(eval_loader.sampler)   \n","    return {\n","                'dice coef':final_dice_coef/len(eval_loader), \n","                'loss':running_loss}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(model, optimizer, criterion, scheduler, train_loader, \n","          valid_loader,device = device,\n","          num_epochs = epochs,\n","          valid_loss_min = np.inf):\n","    \n","    train_loss_list = []\n","    train_dice_coef = []\n","    val_loss_list = []\n","    val_dice_coef = []\n","    \n","    for e in range(num_epochs):\n","        \n","        train_metrics = train_loop(model, optimizer, criterion, train_loader, device=device)\n","        \n","        val_metrics = eval_loop(model, criterion, valid_loader, device=device)\n","        \n","        scheduler.step(val_metrics['dice coef'])\n","        \n","        train_loss_list.append(train_metrics['loss']) \n","        train_dice_coef.append(train_metrics['dice coef'])\n","        val_loss_list.append(val_metrics['loss'])\n","        val_dice_coef.append(val_metrics['dice coef'])\n","        \n","        print_string = f\"Epoch: {e+1}\\n\"\n","        print_string += f\"Train Loss: {train_metrics['loss']:.5f}\\n\"\n","        print_string += f\"Train Dice Coef: {train_metrics['dice coef']:.5f}\\n\"\n","        print_string += f\"Valid Loss: {val_metrics['loss']:.5f}\\n\"\n","        print_string += f\"Valid Dice Coef: {val_metrics['dice coef']:.5f}\\n\"\n","        print(print_string)\n","        \n","        # save model\n","        if val_metrics[\"loss\"] <= valid_loss_min:\n","            torch.save(model.state_dict(), \"UNET.pt\")\n","            valid_loss_min = val_metrics[\"loss\"]\n","    return [train_loss_list,\n","    train_dice_coef,\n","    val_loss_list,\n","    val_dice_coef]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# optimizer = optim.Adam(model.parameters(), lr=0.01)\n","# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n","# # criterion = nn.BCELoss(reduction='mean')\n","# criterion = FocalLoss()\n","# train_loss_list, train_dice_coef,val_loss_list,val_dice_coef = train(\n","#     model, optimizer, criterion, scheduler, train_loader, valid_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# yasai_compute_batch_iou(model, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# yasai_compute_batch_iou(model, valid_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(1500):\n","    yasai_show_v2(test_data, i, model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Eval"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_predictions(model, idx, transforms):\n","    img = Image.open(test_df['images'].iloc[idx]).convert('RGB')\n","    mask = Image.open(test_df['masks'].iloc[idx])\n","    \n","    tensor_img = transforms(img)\n","    tensor_img = tensor_img.unsqueeze(0).to(device)\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        pred = model(tensor_img)[0].detach().cpu().numpy()\n","        pred = pred.transpose((1,2,0)).squeeze()\n","        print(np.max(pred))\n","        rounded = np.round(pred)\n","\n","    \n","        \n","    plot_images = {\n","        'Image': img,\n","        'Mask': mask,\n","        'Predicted Mask': pred,\n","        'Predicted Rounded Mask':rounded\n","    }\n","    \n","    fig, ax = plt.subplots(1, 4, figsize=(16,4))\n","    for i, key in enumerate(plot_images.keys()):\n","        ax[i].imshow(plot_images[key])\n","        ax[i].set_title(key)\n","        \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(np.arange(1, epochs + 1), train_loss_list, label=\"train loss\")\n","plt.plot(np.arange(1, epochs + 1), val_loss_list, label=\"val loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.title(\"Training and validation loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(np.arange(1, epochs + 1), train_dice_coef, label=\"train dice score\")\n","plt.plot(np.arange(1, epochs + 1), val_dice_coef, label=\"val dice score\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Dice\")\n","plt.legend()\n","plt.title(\"Training and validation Dice Score\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_predictions(model, 59, transform)\n","plot_predictions(model, 0, transform)\n","plot_predictions(model, 26, transform)\n","plot_predictions(model, 3, transform)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}}},"nbformat":4,"nbformat_minor":0}
